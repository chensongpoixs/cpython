{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一， jupyter工具的安装\n",
    "\n",
    "1. 在命令行下可用pip下载并安装\n",
    "\n",
    "```\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "2. 安装完成后，在命令行下输入\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "3. 可启动浏览器，进入Web程序界面\n",
    "\n",
    "### 二， 也可以通过安装ipython交互式计算和开发环境(增强的交互式Python shell)，分以下两步安装\n",
    "\n",
    "```\n",
    "pip install ipython\n",
    "\n",
    "pip install “ipython[notebook]”\n",
    "```\n",
    "\n",
    "1. 然后启动应用\n",
    "\n",
    "```\n",
    "ipython notebook\n",
    "```\n",
    "\n",
    "2. 可以通过Upload上载本地文件，如果要直接打开本地的.ipynb文件，可以直接在命令行输入\n",
    "\n",
    "\n",
    "```\n",
    "jupyter notebook e:\\1.ipynb\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三， 数学公式的测试\n",
    "\n",
    "1. 区别\n",
    "\n",
    "PCA 和 线性回归的区别是：\n",
    "\n",
    "\n",
    "![](https://img.halfrost.com/Blog/ArticleImage/78_3_.png)\n",
    "\n",
    "线性回归找的是垂直于 X 轴距离最小值，PCA 找的是投影垂直距离最小值。\n",
    "\n",
    "线性回归目的是想通过 x 预测 y，但是 PCA 的目的是为了找一个降维的面，没有什么特殊的 y，代表降维的面的向量 $x_1$、$x_2$、$x_3$、$x_n$ 都是同等地位的。\n",
    "\n",
    "\n",
    "2. 算法流程\n",
    "\n",
    "假定我们需要将特征维度从 n 维降到 k 维。则 PCA 的执行流程如下：\n",
    "\n",
    "特征标准化，平衡各个特征尺度：\n",
    "\n",
    "$$x^{(i)}_j=\\frac{x^{(i)}_j-\\mu_j}{s_j}$$\n",
    "\n",
    "$\\mu_j$ 为特征 j 的均值，sj 为特征 j 的标准差。\n",
    " \n",
    "计算协方差矩阵 $\\Sigma $ ：\n",
    "\n",
    "\n",
    "$$\\Sigma =\\frac{1}{m}\\sum_{i=1}{m}(x^{(i)})(x^{(i)})^T=\\frac{1}{m} \\cdot  X^TX$$\n",
    " \n",
    "通过奇异值分解（SVD），求取 $\\Sigma $  的特征向量（eigenvectors）：\n",
    "\n",
    "$$(U,S,V^T)=SVD(\\Sigma )$$\n",
    "\n",
    "\n",
    " \n",
    "从 U 中取出前 k 个左奇异向量，构成一个约减矩阵  Ureduce :\n",
    "\n",
    "$$U_{reduce}=(\\mu^{(1)},\\mu^{(2)},\\cdots,\\mu^{(k)})$$\n",
    " \n",
    "计算新的特征向量： $z^{(i)}$ \n",
    "\n",
    "$$z^{(i)}=U^{T}_{reduce} \\cdot  x^{(i)}$$\n",
    "\n",
    "\n",
    "3. 特征还原\n",
    "\n",
    "因为 PCA 仅保留了特征的主成分，所以 PCA 是一种有损的压缩方式，假定我们获得新特征向量为：\n",
    "\n",
    "$$z=U^T_{reduce}x$$\n",
    " \n",
    "那么，还原后的特征 $x_{approx}$ 为：\n",
    "\n",
    "$$x_{approx}=U_{reduce}z$$\n",
    "\n",
    "\n",
    "![](https://img.halfrost.com/Blog/ArticleImage/78_6.png)\n",
    "\n",
    "\n",
    " 4. 降维多少才合适？\n",
    "\n",
    "\n",
    "从 PCA 的执行流程中，我们知道，需要为 PCA 指定目的维度 k 。如果降维不多，则性能提升不大；如果目标维度太小，则又丢失了许多信息。通常，使用如下的流程的来评估 k 值选取优异：\n",
    "\n",
    "求各样本的投影均方误差:\n",
    "\n",
    "$$\\min \\frac{1}{m}\\sum_{j=1}^{m}\\left \\| x^{(i)}-x^{(i)}_{approx} \\right \\|^2$$\n",
    " \n",
    "求数据的总变差：\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{j=1}^{m}\\left \\| x^{(i)} \\right \\|^2$$\n",
    " \n",
    "评估下式是否成立:\n",
    "\n",
    "$$\\frac{\\min \\frac{1}{m}\\sum_{j=1}^{m}\\left \\| x^{(i)}-x^{(i)}_{approx} \\right \\|^2}{\\frac{1}{m}\\sum_{j=1}^{m}\\left \\| x^{(i)} \\right \\|^2} \\leqslant \\epsilon $$\n",
    " \n",
    "其中， $\\epsilon $  的取值可以为  0.01,0.05,0.10,⋯，假设  $\\epsilon = 0.01 $ ，我们就说“特征间 99% 的差异性得到保留”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
